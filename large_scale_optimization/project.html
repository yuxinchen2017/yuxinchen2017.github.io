<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>STAT9910-303: Large-Scale Optimization for Data Science</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">STAT 9910-303</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="course_info.html">Course&nbsp;Info</a></div>
<div class="menu-item"><a href="syllabus.html">Syllabus</a></div>
<div class="menu-item"><a href="lectures.html">Lectures</a></div>
<div class="menu-item"><a href="project.html">Project</a></div>
<div class="menu-item"><a href="reference.html">Reference</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>STAT9910-303: Large-Scale Optimization for Data Science</h1>
<div id="subtitle"><a href="https://yuxinchen2020.github.io/">Yuxin Chen</a>, University of Pennsylvania, Fall 2023</div>
</div>
<p>The term project can either be a literature review or include original research:</p>
<ul>
<li><p><b>Literature review.</b> We will provide a list of related papers not covered in the lectures, and the literature review should involve in-depth summaries and exposition of one of these papers.</p>
</li>
<li><p><b>Original research.</b> It can be either theoretic or experimental (ideally a mix of the two), with approval from the instructor. If you choose this option, you can do it <i>either individually or in groups of two</i>. You are encouraged to combine your current research with your term project.</p>
</li>
</ul>
<p>There are 2 milestones / deliverables to help you through the process.</p>
<ol>
<li><p><b>Proposal</b> (due Oct. 27). Submit a short report (no more than 1 page) stating the papers you plan to survey or the research problems that you plan to work on. Describe why they are important or interesting, and provide some appropriate references. If you elect to do original research, please do not propose an overly ambitious project that cannot be completed by the end of the semester, and do not be too lured by generality. Focus on the simplest scenarios that can capture the issues you’d like to address.
</p>
</li>
<li><p><b>A written report</b> (due Dec. 15). You are expected to submit a final project report &ndash; up to 5 pages with unlimited appendix—summarizing your findings. 
</p>
</li>
</ol>
<p><b>A few suggested (theoretical) papers for literature review (to be updated)</b> </p>
<ol>
<li><p>&lsquo;&lsquo;On the theory of policy gradient methods: Optimality, approximation, and distribution shift,&rsquo;&rsquo; A. Agarwal, S. Kakade, J. Lee, G. Mahajan, Journal of Machine Learning Research, 2021. </p>
</li>
<li><p>&lsquo;&lsquo;Fast global convergence of natural policy gradient methods with entropy regularization,&rsquo;&rsquo; S. Cen, C. Cheng, Y. Chen, Y. Wei, Y. Chi, Operations Research, 2022. </p>
</li>
<li><p>&lsquo;&lsquo;Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes,&rsquo; G. Lan, Mathematical Programming, 2023. </p>
</li>
<li><p>&lsquo;&lsquo;Optimization, learning, and games with predictable sequences,&rsquo;&rsquo; A. Rakhlin, K Sridharan, NeurIPS 2013. </p>
</li>
<li><p>&lsquo;&lsquo;On the convergence of fedavg on non-iid data,&rsquo;&rsquo; X. Li, K. Huang, W. Yang, S. Wang, Z. Zhang, ICLR 2019.  </p>
</li>
<li><p>&lsquo;&lsquo;On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems,&rsquo;&rsquo; T. Lin, J. Chi, and M. Jordan, 2019. </p>
</li>
<li><p>&lsquo;&lsquo;On the convergence of Adam and beyond,&rsquo;&rsquo; S. Reddi, S. Kale, S. Kumar, 2019. </p>
</li>
<li><p>&lsquo;&lsquo;On-Demand Sampling: Learning Optimally from Multiple Distributions,&rsquo;&rsquo; N. Haghtalab, M. Jordan, E. Zhao, NeurIPS 2022. </p>
</li>
<li><p>&lsquo;&lsquo;Minimax Regret Optimization for Robust Machine Learning under Distribution Shift,&rsquo;&rsquo; A. Agarwal, T. Zhang, 2022. </p>
</li>
<li><p>&lsquo;&lsquo;FedPD: A Federated Learning Framework With Adaptivity to Non-IID Data,&rsquo;&rsquo; X. Zhang, M. Hong, S. Dhople, W. Yin, Y. Liu, IEEE Transactions on Signal Processing, 2021. </p>
</li>
<li><p>&lsquo;&lsquo;Bipartite Matching in Nearly-linear Time on Moderately Dense Graphs,&rsquo;&rsquo; J. van den Brand, Y. T. Lee, D. Nanongkai, R. Peng, T. Saranurak, A. Sidford, Z. Song, D. Wang,  FOCS 2020. </p>
</li>
<li><p>&lsquo;&lsquo;A two-timescale stochastic algorithm framework for bilevel optimization: Complexity analysis and application to actor-critic,&rsquo;&rsquo; M. Hong, H. Wai, Z. Wang, Z. Yang, SIAM Journal on Optimization, 2023. </p>
</li>
<li><p>&lsquo;&lsquo;Further and stronger analogy between sampling and optimization: Langevin Monte Carlo and gradient descent,&rsquo;&rsquo; A. Dalalyan, Conference on Learning Theory, 2017. </p>
</li>
<li><p>&lsquo;&lsquo;Sharp analysis for nonconvex sgd escaping from saddle points,&rsquo;&rsquo;  C. Fang, Z. Lin, and T. Zhang, 2019. 
</p>
</li>
<li><p>&lsquo;&lsquo;Stochastic model-based minimization of weakly convex functions,&rsquo;&rsquo; D. Davis, D. Drusvyatskiy, SIAM Journal on Optimization, 2019.</p>
</li>
<li><p>&lsquo;&lsquo;Stochastic methods for composite and weakly convex optimization problems,&rsquo;&rsquo; J. Duchi, R. Feng, SIAM Journal on Optimization, 2018. </p>
</li>
<li><p>&lsquo;&lsquo;EXTRA: An Exact First-Order Algorithm for Decentralized Consensus Optimization,&rsquo;&rsquo; W. Shi, Q. Ling, G. Wu, W. Yin, 2014. </p>
</li>
<li><p>&lsquo;&lsquo;Convergence Analysis of Alternating Direction Method of Multipliers for a Family of Nonconvex Problems, &rsquo;&rsquo; M. Hong, Z.Q. Luo, and M. Razaviyayn, 2016</p>
</li>
<li><p>&lsquo;&lsquo;A Geometric Alternative to Nesterov's Accelerated Gradient Descent, &rsquo;&rsquo; S. Bubeck, Y. T. Lee, M Singh, 2015</p>
</li>
<li><p>&lsquo;&lsquo;Implicit Regularization in Nonconvex Statistical Estimation: Gradient Descent Converges Linearly for Phase Retrieval, Matrix Completion and Blind Deconvolution, &rsquo;&rsquo; C. Ma, K. Wang, Y. Chi, and Y. Chen, Foundations of Computational Mathematics, 2020</p>
</li>
<li><p>&lsquo;&lsquo;Gradient Primal-Dual Algorithm Converges to Second-Order Stationary Solutions for Nonconvex Distributed Optimization, &rsquo;&rsquo; M. Hong, J. Lee, M Razaviyayn, 2018</p>
</li>
<li><p>&lsquo;&lsquo;Mirror descent in non-convex stochastic programming, &rsquo;&rsquo; Z. Zhou, P. Mertikopoulos, N. Bambos, S. Boyd, P. Glynn, 2017</p>
</li>
<li><p>&lsquo;&lsquo;Gradient Descent Can Take Exponential Time to Escape Saddle Points, &rsquo;&rsquo; S. Du, C. Jin, J. Lee, M. Jordan, B. Poczos, A. Singh, 2017</p>
</li>
<li><p>&lsquo;&lsquo;Accelerating Stochastic Gradient Descent, &rsquo;&rsquo; P. Jain, S. Kakade, R. Kidambi, P. Netrapalli, A. Sidford, 2017</p>
</li>
</ol>
<p><i>You have the freedom to select a paper of your own interest (especially more practical papers), upon the instructor's approval. </i></p>
<div id="footer">
<div id="footer-text">
Page generated 2023-09-07 14:45:40 EDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
